{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('data/ratings_train.txt')\n",
    "test_data = pd.read_table('data/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 43752\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
      "단어 집합에서 희귀 단어의 비율: 55.62488571950996\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.8715872104872904\n"
     ]
    }
   ],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 19417\n"
     ]
    }
   ],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145380\n",
      "145380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kebhana/opt/anaconda3/envs/py-nlp/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "dropout_prob = (0.5, 0.8)\n",
    "num_filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape = (max_len,))\n",
    "z = Embedding(vocab_size, embedding_dim, input_length = max_len, name='embedding')(model_input)\n",
    "z = Dropout(dropout_prob[0])(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_blocks = []\n",
    "\n",
    "for sz in [3,4,5]:\n",
    "    conv = Conv1D(filters = num_filters,\n",
    "                kernel_size=sz,\n",
    "                padding='valid',\n",
    "                activation='relu',\n",
    "                strides=1)(z)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "model_output = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2272/2272 [==============================] - ETA: 0s - loss: 0.3953 - acc: 0.8234\n",
      "Epoch 00001: val_acc improved from -inf to 0.83943, saving model to model/1d_cnn_naver_movie.h5\n",
      "2272/2272 [==============================] - 86s 38ms/step - loss: 0.3953 - acc: 0.8234 - val_loss: 0.3657 - val_acc: 0.8394\n",
      "Epoch 2/10\n",
      "2271/2272 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8550\n",
      "Epoch 00002: val_acc improved from 0.83943 to 0.84313, saving model to model/1d_cnn_naver_movie.h5\n",
      "2272/2272 [==============================] - 84s 37ms/step - loss: 0.3400 - acc: 0.8550 - val_loss: 0.3579 - val_acc: 0.8431\n",
      "Epoch 3/10\n",
      "2271/2272 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8681\n",
      "Epoch 00003: val_acc did not improve from 0.84313\n",
      "2272/2272 [==============================] - 85s 37ms/step - loss: 0.3129 - acc: 0.8681 - val_loss: 0.3572 - val_acc: 0.8419\n",
      "Epoch 4/10\n",
      "2271/2272 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.8783\n",
      "Epoch 00004: val_acc did not improve from 0.84313\n",
      "2272/2272 [==============================] - 85s 37ms/step - loss: 0.2918 - acc: 0.8783 - val_loss: 0.3606 - val_acc: 0.8423\n",
      "Epoch 5/10\n",
      "2271/2272 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.8872\n",
      "Epoch 00005: val_acc did not improve from 0.84313\n",
      "2272/2272 [==============================] - 85s 37ms/step - loss: 0.2748 - acc: 0.8872 - val_loss: 0.3636 - val_acc: 0.8415\n",
      "Epoch 6/10\n",
      "2272/2272 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.8934\n",
      "Epoch 00006: val_acc did not improve from 0.84313\n",
      "2272/2272 [==============================] - 86s 38ms/step - loss: 0.2600 - acc: 0.8934 - val_loss: 0.3688 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "2272/2272 [==============================] - ETA: 0s - loss: 0.2470 - acc: 0.8988\n",
      "Epoch 00007: val_acc did not improve from 0.84313\n",
      "2272/2272 [==============================] - 93s 41ms/step - loss: 0.2470 - acc: 0.8988 - val_loss: 0.3760 - val_acc: 0.8403\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96dcbe0eb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor ='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('model/1d_cnn_naver_movie.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test), verbose=1, callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532/1532 [==============================] - 4s 3ms/step - loss: 0.3579 - acc: 0.8431\n",
      "\n",
      " 테스트 정확도: 0.8431\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('model/1d_cnn_naver_movie.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem = True)\n",
    "    new_sentence = [word for word in new_sentence if word not in stopwords]\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "    pad_new = pad_sequences(encoded, maxlen =max_len)\n",
    "    score = float(model.predict(pad_new))\n",
    "    if (score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰\".format(score*100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰\".format((1-score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.35% 확률로 긍정 리뷰\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict(\"이 영화 개꿀잼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.04% 확률로 긍정 리뷰\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict(\"시간 가는줄 모르고 봄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.46% 확률로 부정 리뷰\n"
     ]
    }
   ],
   "source": [
    "sentiment_predict('감독 뭐하는 놈이냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
