{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178009, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('data/fra.txt', names=['src','tar','lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    src       tar\n",
       "0   Go.      Va !\n",
       "1   Hi.   Salut !\n",
       "2   Hi.    Salut.\n",
       "3  Run!   Cours !\n",
       "4  Run!  Courez !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17440</th>\n",
       "      <td>Where is my seat?</td>\n",
       "      <td>Où est mon siège ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34141</th>\n",
       "      <td>Do you do that often?</td>\n",
       "      <td>Tu fais ça souvent ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>I want to walk.</td>\n",
       "      <td>Je veux marcher.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12717</th>\n",
       "      <td>This is on sale.</td>\n",
       "      <td>C'est en promo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15326</th>\n",
       "      <td>I will try again.</td>\n",
       "      <td>J'essaierai à nouveau.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59438</th>\n",
       "      <td>Why wouldn't we do that?</td>\n",
       "      <td>Pourquoi ne ferions-nous pas cela ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38919</th>\n",
       "      <td>We have to warn them.</td>\n",
       "      <td>Il faut que nous les avertissions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23195</th>\n",
       "      <td>Do you like robots?</td>\n",
       "      <td>Aimes-tu les robots ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26930</th>\n",
       "      <td>Tom was a crusader.</td>\n",
       "      <td>Tom était un croisé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43057</th>\n",
       "      <td>I'm not entirely sure.</td>\n",
       "      <td>Je ne suis pas entièrement sûr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                  tar\n",
       "17440         Where is my seat?                   Où est mon siège ?\n",
       "34141     Do you do that often?                 Tu fais ça souvent ?\n",
       "8325            I want to walk.                     Je veux marcher.\n",
       "12717          This is on sale.                      C'est en promo.\n",
       "15326         I will try again.               J'essaierai à nouveau.\n",
       "59438  Why wouldn't we do that?  Pourquoi ne ferions-nous pas cela ?\n",
       "38919     We have to warn them.   Il faut que nous les avertissions.\n",
       "23195       Do you like robots?                Aimes-tu les robots ?\n",
       "26930       Tom was a crusader.                 Tom était un croisé.\n",
       "43057    I'm not entirely sure.      Je ne suis pas entièrement sûr."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25634</th>\n",
       "      <td>Look at my new car.</td>\n",
       "      <td>\\t Regardez ma nouvelle voiture. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>I remember them.</td>\n",
       "      <td>\\t Je me souviens d'elles. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33191</th>\n",
       "      <td>Who's replacing you?</td>\n",
       "      <td>\\t Qui te remplace ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34993</th>\n",
       "      <td>His teeth were white.</td>\n",
       "      <td>\\t Ses dents furent blanches. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59928</th>\n",
       "      <td>You're not missing much.</td>\n",
       "      <td>\\t Tu ne rates pas grand chose. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53017</th>\n",
       "      <td>You need to be careful.</td>\n",
       "      <td>\\t Il faut que tu sois prudent. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47298</th>\n",
       "      <td>Don't try to be a hero.</td>\n",
       "      <td>\\t Ne joue pas les héros ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32059</th>\n",
       "      <td>They know your name.</td>\n",
       "      <td>\\t Ils connaissent votre nom. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41696</th>\n",
       "      <td>I couldn't lie to you.</td>\n",
       "      <td>\\t Je ne pourrais vous mentir. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25047</th>\n",
       "      <td>I'm in trouble now.</td>\n",
       "      <td>\\t Désormais, j'ai des ennuis. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                  tar\n",
       "25634       Look at my new car.  \\t Regardez ma nouvelle voiture. \\n\n",
       "11308          I remember them.        \\t Je me souviens d'elles. \\n\n",
       "33191      Who's replacing you?              \\t Qui te remplace ? \\n\n",
       "34993     His teeth were white.     \\t Ses dents furent blanches. \\n\n",
       "59928  You're not missing much.   \\t Tu ne rates pas grand chose. \\n\n",
       "53017   You need to be careful.   \\t Il faut que tu sois prudent. \\n\n",
       "47298   Don't try to be a hero.        \\t Ne joue pas les héros ! \\n\n",
       "32059      They know your name.     \\t Ils connaissent votre nom. \\n\n",
       "41696    I couldn't lie to you.    \\t Je ne pourrais vous mentir. \\n\n",
       "25047       I'm in trouble now.    \\t Désormais, j'ai des ennuis. \\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#글자 집합 구축\n",
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "        \n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a']\n",
      "['\\t', '\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[:50])\n",
    "print(tar_vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "      if t>0:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "      t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs도 같이 리턴받기는 했지만 여기서는 필요없으므로 이 값은 버림.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 바로 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 157s 210ms/step - loss: 0.7812 - val_loss: 0.6810\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 160s 214ms/step - loss: 0.4833 - val_loss: 0.5646\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 152s 202ms/step - loss: 0.3996 - val_loss: 0.4837\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 154s 205ms/step - loss: 0.3520 - val_loss: 0.4429\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 157s 210ms/step - loss: 0.3209 - val_loss: 0.4152\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 153s 205ms/step - loss: 0.2988 - val_loss: 0.3977\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 156s 208ms/step - loss: 0.2821 - val_loss: 0.3846\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 154s 206ms/step - loss: 0.2686 - val_loss: 0.3769\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 152s 203ms/step - loss: 0.2574 - val_loss: 0.3699\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 153s 204ms/step - loss: 0.2479 - val_loss: 0.3645\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 157s 209ms/step - loss: 0.2394 - val_loss: 0.3605\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 151s 202ms/step - loss: 0.2318 - val_loss: 0.3591\n",
      "Epoch 13/50\n",
      "750/750 [==============================] - 153s 204ms/step - loss: 0.2250 - val_loss: 0.3567\n",
      "Epoch 14/50\n",
      "750/750 [==============================] - 160s 213ms/step - loss: 0.2187 - val_loss: 0.3548\n",
      "Epoch 15/50\n",
      "750/750 [==============================] - 161s 214ms/step - loss: 0.2130 - val_loss: 0.3541\n",
      "Epoch 16/50\n",
      "750/750 [==============================] - 154s 205ms/step - loss: 0.2078 - val_loss: 0.3545\n",
      "Epoch 17/50\n",
      "750/750 [==============================] - 151s 201ms/step - loss: 0.2028 - val_loss: 0.3549\n",
      "Epoch 18/50\n",
      "750/750 [==============================] - 151s 201ms/step - loss: 0.1984 - val_loss: 0.3547\n",
      "Epoch 19/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1940 - val_loss: 0.3568\n",
      "Epoch 20/50\n",
      "750/750 [==============================] - 154s 205ms/step - loss: 0.1900 - val_loss: 0.3567\n",
      "Epoch 21/50\n",
      "750/750 [==============================] - 155s 207ms/step - loss: 0.1863 - val_loss: 0.3588\n",
      "Epoch 22/50\n",
      "750/750 [==============================] - 154s 205ms/step - loss: 0.1826 - val_loss: 0.3596\n",
      "Epoch 23/50\n",
      "750/750 [==============================] - 152s 202ms/step - loss: 0.1792 - val_loss: 0.3616\n",
      "Epoch 24/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1760 - val_loss: 0.3650\n",
      "Epoch 25/50\n",
      "750/750 [==============================] - 155s 207ms/step - loss: 0.1728 - val_loss: 0.3668\n",
      "Epoch 26/50\n",
      "750/750 [==============================] - 157s 210ms/step - loss: 0.1699 - val_loss: 0.3679\n",
      "Epoch 27/50\n",
      "750/750 [==============================] - 170s 226ms/step - loss: 0.1671 - val_loss: 0.3721\n",
      "Epoch 28/50\n",
      "750/750 [==============================] - 161s 214ms/step - loss: 0.1643 - val_loss: 0.3751\n",
      "Epoch 29/50\n",
      "750/750 [==============================] - 155s 207ms/step - loss: 0.1617 - val_loss: 0.3771\n",
      "Epoch 30/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1593 - val_loss: 0.3803\n",
      "Epoch 31/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1570 - val_loss: 0.3811\n",
      "Epoch 32/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1547 - val_loss: 0.3860\n",
      "Epoch 33/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1524 - val_loss: 0.3867\n",
      "Epoch 34/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1503 - val_loss: 0.3891\n",
      "Epoch 35/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1483 - val_loss: 0.3916\n",
      "Epoch 36/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1464 - val_loss: 0.3937\n",
      "Epoch 37/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1444 - val_loss: 0.3982\n",
      "Epoch 38/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1426 - val_loss: 0.3996\n",
      "Epoch 39/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1411 - val_loss: 0.4032\n",
      "Epoch 40/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1394 - val_loss: 0.4064\n",
      "Epoch 41/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1376 - val_loss: 0.4089\n",
      "Epoch 42/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1361 - val_loss: 0.4123\n",
      "Epoch 43/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1345 - val_loss: 0.4149\n",
      "Epoch 44/50\n",
      "750/750 [==============================] - 150s 201ms/step - loss: 0.1331 - val_loss: 0.4163\n",
      "Epoch 45/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1317 - val_loss: 0.4181\n",
      "Epoch 46/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1303 - val_loss: 0.4211\n",
      "Epoch 47/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1291 - val_loss: 0.4262\n",
      "Epoch 48/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1278 - val_loss: 0.4260\n",
      "Epoch 49/50\n",
      "750/750 [==============================] - 150s 200ms/step - loss: 0.1264 - val_loss: 0.4298\n",
      "Epoch 50/50\n",
      "750/750 [==============================] - 150s 199ms/step - loss: 0.1253 - val_loss: 0.4314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd5adbf40d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Coursez ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  Je suis tombée. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  Appelez-nous ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  Comme c'est agréable ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  Tourne la télé. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
